{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import aiohttp\n",
    "import asyncio\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### code to get all 20_000 links in this\n",
    "\n",
    "Takes about 14 minutes\n",
    "\n",
    "Although, there appears to be other links that are not sequential. such as Councilor Rory O'Connor: https://www.irelandelection.com/candidate.php?candid=18829"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "CancelledError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mCancelledError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [3], line 44\u001b[0m\n\u001b[0;32m     42\u001b[0m connector \u001b[39m=\u001b[39m aiohttp\u001b[39m.\u001b[39mTCPConnector(limit\u001b[39m=\u001b[39m\u001b[39m60\u001b[39m)\n\u001b[0;32m     43\u001b[0m \u001b[39masync\u001b[39;00m \u001b[39mwith\u001b[39;00m aiohttp\u001b[39m.\u001b[39mClientSession(connector\u001b[39m=\u001b[39mconnector,\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mclient_args) \u001b[39mas\u001b[39;00m s:\n\u001b[1;32m---> 44\u001b[0m     responses \u001b[39m=\u001b[39m \u001b[39mawait\u001b[39;00m get_responses(s)\n",
      "Cell \u001b[1;32mIn [3], line 38\u001b[0m, in \u001b[0;36mget_responses\u001b[1;34m(session)\u001b[0m\n\u001b[0;32m     36\u001b[0m     url \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mhttps://www.irelandelection.com/candidate.php?candid=\u001b[39m\u001b[39m{\u001b[39;00mi\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\n\u001b[0;32m     37\u001b[0m     tasks\u001b[39m.\u001b[39mappend(fetch(session,url))\n\u001b[1;32m---> 38\u001b[0m responses \u001b[39m=\u001b[39m \u001b[39mawait\u001b[39;00m asyncio\u001b[39m.\u001b[39mgather(\u001b[39m*\u001b[39mtasks, return_exceptions\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m     39\u001b[0m \u001b[39mreturn\u001b[39;00m responses\n",
      "Cell \u001b[1;32mIn [3], line 17\u001b[0m, in \u001b[0;36mfetch\u001b[1;34m(session, url)\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[39masync\u001b[39;00m \u001b[39mdef\u001b[39;00m \u001b[39mfetch\u001b[39m(session,url):\n\u001b[0;32m     16\u001b[0m     \u001b[39m\"\"\"Fetch a url, using specified ClientSession.\"\"\"\u001b[39;00m\n\u001b[1;32m---> 17\u001b[0m     \u001b[39masync\u001b[39;00m \u001b[39mwith\u001b[39;00m session\u001b[39m.\u001b[39mget(url) \u001b[39mas\u001b[39;00m response:\n\u001b[0;32m     18\u001b[0m         \u001b[39m# print(f\"fetching {url}\")\u001b[39;00m\n\u001b[0;32m     19\u001b[0m         \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     20\u001b[0m             resp \u001b[39m=\u001b[39m \u001b[39mawait\u001b[39;00m response\u001b[39m.\u001b[39mread()\n",
      "File \u001b[1;32mc:\\Users\\Rober\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\aiohttp\\client.py:1141\u001b[0m, in \u001b[0;36m_BaseRequestContextManager.__aenter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1140\u001b[0m \u001b[39masync\u001b[39;00m \u001b[39mdef\u001b[39;00m \u001b[39m__aenter__\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m _RetType:\n\u001b[1;32m-> 1141\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_resp \u001b[39m=\u001b[39m \u001b[39mawait\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_coro\n\u001b[0;32m   1142\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_resp\n",
      "File \u001b[1;32mc:\\Users\\Rober\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\aiohttp\\client.py:536\u001b[0m, in \u001b[0;36mClientSession._request\u001b[1;34m(self, method, str_or_url, params, data, json, cookies, headers, skip_auto_headers, auth, allow_redirects, max_redirects, compress, chunked, expect100, raise_for_status, read_until_eof, proxy, proxy_auth, timeout, verify_ssl, fingerprint, ssl_context, ssl, proxy_headers, trace_request_ctx, read_bufsize)\u001b[0m\n\u001b[0;32m    534\u001b[0m     \u001b[39masync\u001b[39;00m \u001b[39mwith\u001b[39;00m ceil_timeout(real_timeout\u001b[39m.\u001b[39mconnect):\n\u001b[0;32m    535\u001b[0m         \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_connector \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m--> 536\u001b[0m         conn \u001b[39m=\u001b[39m \u001b[39mawait\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_connector\u001b[39m.\u001b[39mconnect(\n\u001b[0;32m    537\u001b[0m             req, traces\u001b[39m=\u001b[39mtraces, timeout\u001b[39m=\u001b[39mreal_timeout\n\u001b[0;32m    538\u001b[0m         )\n\u001b[0;32m    539\u001b[0m \u001b[39mexcept\u001b[39;00m asyncio\u001b[39m.\u001b[39mTimeoutError \u001b[39mas\u001b[39;00m exc:\n\u001b[0;32m    540\u001b[0m     \u001b[39mraise\u001b[39;00m ServerTimeoutError(\n\u001b[0;32m    541\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mConnection timeout \u001b[39m\u001b[39m\"\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mto host \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(url)\n\u001b[0;32m    542\u001b[0m     ) \u001b[39mfrom\u001b[39;00m \u001b[39mexc\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Rober\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\aiohttp\\connector.py:520\u001b[0m, in \u001b[0;36mBaseConnector.connect\u001b[1;34m(self, req, traces, timeout)\u001b[0m\n\u001b[0;32m    517\u001b[0m         \u001b[39mexcept\u001b[39;00m \u001b[39mValueError\u001b[39;00m:  \u001b[39m# fut may no longer be in list\u001b[39;00m\n\u001b[0;32m    518\u001b[0m             \u001b[39mpass\u001b[39;00m\n\u001b[1;32m--> 520\u001b[0m     \u001b[39mraise\u001b[39;00m e\n\u001b[0;32m    521\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m    522\u001b[0m     \u001b[39mif\u001b[39;00m key \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_waiters \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_waiters[key]:\n",
      "File \u001b[1;32mc:\\Users\\Rober\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\aiohttp\\connector.py:510\u001b[0m, in \u001b[0;36mBaseConnector.connect\u001b[1;34m(self, req, traces, timeout)\u001b[0m\n\u001b[0;32m    507\u001b[0m         \u001b[39mawait\u001b[39;00m trace\u001b[39m.\u001b[39msend_connection_queued_start()\n\u001b[0;32m    509\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 510\u001b[0m     \u001b[39mawait\u001b[39;00m fut\n\u001b[0;32m    511\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mBaseException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    512\u001b[0m     \u001b[39mif\u001b[39;00m key \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_waiters:\n\u001b[0;32m    513\u001b[0m         \u001b[39m# remove a waiter even if it was cancelled, normally it's\u001b[39;00m\n\u001b[0;32m    514\u001b[0m         \u001b[39m#  removed when it's notified\u001b[39;00m\n",
      "\u001b[1;31mCancelledError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "failed = []\n",
    "my_timeout = aiohttp.ClientTimeout(\n",
    "    total=None, # default value is 5 minutes, set to `None` for unlimited timeout\n",
    "    sock_connect=150, # How long to wait before an open socket allowed to connect\n",
    "    sock_read=150 # How long to wait with no data being read before timing out\n",
    ")\n",
    "\n",
    "client_args = dict(\n",
    "    trust_env=True,\n",
    "    timeout=my_timeout\n",
    ")\n",
    "\n",
    " \n",
    "\n",
    "async def fetch(session,url):\n",
    "    \"\"\"Fetch a url, using specified ClientSession.\"\"\"\n",
    "    async with session.get(url) as response:\n",
    "        # print(f\"fetching {url}\")\n",
    "        try:\n",
    "            resp = await response.read()\n",
    "            return (url,resp)\n",
    "\n",
    "        except asyncio.TimeoutError:\n",
    "            failed.append(url)\n",
    "            print('timeout')\n",
    "            return {\"results\": f\"timeout error on {url}\"}\n",
    "\n",
    "        if response.status != 200:\n",
    "            failed.append(url)\n",
    "            print('error')\n",
    "            return {\"error\": f\"server returned {response.status}\"}\n",
    "\n",
    "async def get_responses(session):\n",
    "    tasks = []\n",
    "    for i in range(1,20_000):\n",
    "        url = f'https://www.irelandelection.com/candidate.php?candid={i}'\n",
    "        tasks.append(fetch(session,url))\n",
    "    responses = await asyncio.gather(*tasks, return_exceptions=True)\n",
    "    return responses\n",
    "\n",
    "            \n",
    "connector = aiohttp.TCPConnector(limit=60)\n",
    "async with aiohttp.ClientSession(connector=connector,**client_args) as s:\n",
    "    responses = await get_responses(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(failed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed on these links:\n",
      " []\n"
     ]
    }
   ],
   "source": [
    "print('Failed on these links:\\n',failed)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions for getting data from the page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "def get_candidate_name(soup):\n",
    "    name = soup.find('title').text.split(' - ')[0].strip()\n",
    "    return name\n",
    "\n",
    "def get_election_type(election_string):\n",
    "    election_string = election_string.lower()\n",
    "    if 'local election' in election_string:\n",
    "        return 'LOCAL'\n",
    "    elif 'general election' in election_string:\n",
    "        return 'GENERAL'\n",
    "    elif 'european election' in election_string:\n",
    "        return 'EUROPEAN'\n",
    "    elif 'presidential election' in election_string:\n",
    "        return 'PRESIDENTIAL'\n",
    "    elif 'by-election' in election_string:\n",
    "        return 'BI-ELECTION'\n",
    "    \n",
    "def get_constituency_name(election_string):\n",
    "    constituency =  election_string.split(' - ')[1].strip()\n",
    "    return constituency\n",
    "\n",
    "def process_row(row):\n",
    "    d = {}\n",
    "    count = 0\n",
    "    column_names = ['election','first_pref_pct','first_pref_count','first_pref_quota_ratio','elected']\n",
    "    for td in row.find_all('td'):\n",
    "        txt = td.text\n",
    "        if txt:\n",
    "            d[column_names[count]] = txt.strip()\n",
    "            count+=1\n",
    "            d['elected'] = True if td.find('img') else False\n",
    "        elif td.find('img').get('title'):\n",
    "            d['party'] = td.find('img').get('title')\n",
    "    return d\n",
    "    \n",
    "def extract_table(soup):\n",
    "    list_of_rows = []\n",
    "    table_body = soup.find('tbody')\n",
    "    if not table_body:\n",
    "        raise Exception('No Table Found')\n",
    "    rows = table_body.find_all('tr')\n",
    "    for row in rows:\n",
    "        d = process_row(row)\n",
    "        list_of_rows.append(d)\n",
    "    return list_of_rows\n",
    "\n",
    "def create_dataframe(name,list_of_rows):\n",
    "    df = pd.DataFrame(list_of_rows)\n",
    "    df['elected'] = df.elected.fillna(False)\n",
    "    df['year'] = df.election.apply(lambda election_string: int(election_string[:4]))\n",
    "    df['candidate'] = name\n",
    "    df['constituency'] = df.election.apply(get_constituency_name)\n",
    "    df['election_type'] = df.election.apply(get_election_type)\n",
    "    return df\n",
    "\n",
    "def extract_data_from_page(res):\n",
    "    soup = BeautifulSoup(res)\n",
    "    if len(list(soup.stripped_strings)) == 3:\n",
    "        raise Exception('Empty Page')\n",
    "    name = get_candidate_name(soup)\n",
    "    rows = extract_table(soup)\n",
    "    df = create_dataframe(name,rows)\n",
    "    return df\n",
    "\n",
    "#extract_data_from_page()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_dataframes = []\n",
    "fails = []\n",
    "for (url,resp) in responses:\n",
    "    #print(url)\n",
    "    try:\n",
    "        dataframe = extract_data_from_page(resp)\n",
    "        list_of_dataframes.append(dataframe)\n",
    "    except Exception as e: \n",
    "        print('Failed:',url)\n",
    "        print(e,'\\n------------------------')\n",
    "        if str(e) == 'No Table Found':\n",
    "            fails.append((url,resp))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "have to run throught the failed ones before concatinating the dataframes. \n",
    "\n",
    "Fails happen for 2 reasons:\n",
    "- The page is empty eg: https://www.irelandelection.com/candidate.php?candid=30\n",
    "- The page didnt return anything because I sent so many requests that their server couldnt respond.\n",
    "\n",
    "We send the requests back through again for the failed because of lack of tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fails2 = []\n",
    "tasks = []\n",
    "\n",
    "connector = aiohttp.TCPConnector(limit=60)\n",
    "async with aiohttp.ClientSession(connector=connector,**client_args) as s:\n",
    "    for (url,resp) in fails:\n",
    "        tasks.append(fetch(s,url)) \n",
    "    responses2 = await asyncio.gather(*tasks, return_exceptions=True)\n",
    "\n",
    "\n",
    "for (url,resp) in responses2:\n",
    "    #print(url)\n",
    "    try:\n",
    "        dataframe = extract_data_from_page(resp)\n",
    "        list_of_dataframes.append(dataframe)\n",
    "    except Exception as e: \n",
    "        print('Failed:',url)\n",
    "        print(e,'\\n------------------------')\n",
    "        if str(e) == 'No Table Found':\n",
    "            fails2.append(\n",
    "                (url,resp)\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(fails2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DF = pd.concat(list_of_dataframes)\n",
    "DF.to_parquet('ALL_CANDIDATES.parquet')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For every elections we want\n",
    "- Number of Constituencies\n",
    "- How many consituency do we have vote data on?\n",
    "- What was the quota?\n",
    "- What was the votes/quota in first count?\n",
    "- What was the lowest votes/quota?\n",
    "- What was the highest votes/quota?\n",
    "- Who transfered to who (if you have transfer data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "21492c4fe8720d6af13e280c2b801faa6d368fda036b4f197ba5cbd3f78878df"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
