{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import aiohttp\n",
    "import asyncio\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### code to get all 20_000 links in this\n",
    "\n",
    "Takes about 14 minutes\n",
    "\n",
    "Although, there appears to be other links that are not sequential. such as Councilor Rory O'Connor: https://www.irelandelection.com/candidate.php?candid=18829"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "failed = []\n",
    "my_timeout = aiohttp.ClientTimeout(\n",
    "    total=None, # default value is 5 minutes, set to `None` for unlimited timeout\n",
    "    sock_connect=150, # How long to wait before an open socket allowed to connect\n",
    "    sock_read=150 # How long to wait with no data being read before timing out\n",
    ")\n",
    "\n",
    "client_args = dict(\n",
    "    trust_env=True,\n",
    "    timeout=my_timeout\n",
    ")\n",
    "\n",
    " \n",
    "\n",
    "async def fetch(session,url):\n",
    "    \"\"\"Fetch a url, using specified ClientSession.\"\"\"\n",
    "    async with session.get(url) as response:\n",
    "        # print(f\"fetching {url}\")\n",
    "        try:\n",
    "            resp = await response.read()\n",
    "            return (url,resp)\n",
    "\n",
    "        except asyncio.TimeoutError:\n",
    "            failed.append(url)\n",
    "            print('timeout')\n",
    "            return {\"results\": f\"timeout error on {url}\"}\n",
    "\n",
    "        if response.status != 200:\n",
    "            failed.append(url)\n",
    "            print('error')\n",
    "            return {\"error\": f\"server returned {response.status}\"}\n",
    "\n",
    "async def get_responses(session):\n",
    "    tasks = []\n",
    "    for i in range(1,20_000):\n",
    "        url = f'https://www.irelandelection.com/candidate.php?candid={i}'\n",
    "        tasks.append(fetch(session,url))\n",
    "    responses = await asyncio.gather(*tasks, return_exceptions=True)\n",
    "    return responses\n",
    "\n",
    "            \n",
    "connector = aiohttp.TCPConnector(limit=60)\n",
    "async with aiohttp.ClientSession(connector=connector,**client_args) as s:\n",
    "    responses = await get_responses(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(failed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Failed on these links:\\n',failed)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions for getting data from the page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_candidate_name(soup):\n",
    "    name = soup.find('title').text.split(' - ')[0].strip()\n",
    "    return name\n",
    "\n",
    "def get_election_type(election_string):\n",
    "    election_string = election_string.lower()\n",
    "    if 'local election' in election_string:\n",
    "        return 'LOCAL'\n",
    "    elif 'general election' in election_string:\n",
    "        return 'GENERAL'\n",
    "    elif 'european election' in election_string:\n",
    "        return 'EUROPEAN'\n",
    "    elif 'presidential election' in election_string:\n",
    "        return 'PRESIDENTIAL'\n",
    "    elif 'by-election' in election_string:\n",
    "        return 'BI-ELECTION'\n",
    "    \n",
    "def get_constituency_name(election_string):\n",
    "    constituency =  election_string.split(' - ')[1].strip()\n",
    "    return constituency\n",
    "\n",
    "def process_row(row):\n",
    "    d = {}\n",
    "    count = 0\n",
    "    column_names = ['election','first_pref_pct','first_pref_count','first_pref_quota_ratio',]\n",
    "    for td in row.find_all('td'):\n",
    "        txt = td.text.strip()\n",
    "        if txt:\n",
    "            d[column_names[count]] = txt\n",
    "            count+=1\n",
    "            d['elected'] = False if td.find('img') else True\n",
    "        elif td.find('img'):\n",
    "            d['party'] = td.find('img').get('title')\n",
    "    return d\n",
    "    \n",
    "def extract_table(soup):\n",
    "    list_of_rows = []\n",
    "    table_body = soup.find('tbody')\n",
    "    if not table_body:\n",
    "        raise Exception('No Table Found')\n",
    "    rows = table_body.find_all('tr')\n",
    "    for row in rows:\n",
    "        d = process_row(row)\n",
    "        list_of_rows.append(d)\n",
    "    return list_of_rows\n",
    "\n",
    "def create_dataframe(name,list_of_rows):\n",
    "    df = pd.DataFrame(list_of_rows)\n",
    "    df['elected'] = df.elected.fillna(False)\n",
    "    df['year'] = df.election.apply(lambda election_string: int(election_string[:4]))\n",
    "    df['candidate'] = name\n",
    "    df['constituency'] = df.election.apply(get_constituency_name)\n",
    "    df['election_type'] = df.election.apply(get_election_type)\n",
    "    return df\n",
    "\n",
    "def extract_data_from_page(res):\n",
    "    soup = BeautifulSoup(res)\n",
    "    if len(list(soup.stripped_strings)) == 3:\n",
    "        raise Exception('Empty Page')\n",
    "    name = get_candidate_name(soup)\n",
    "    rows = extract_table(soup)\n",
    "    df = create_dataframe(name,rows)\n",
    "    return df\n",
    "\n",
    "#extract_data_from_page()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_dataframes = []\n",
    "fails = []\n",
    "for (url,resp) in responses:\n",
    "    #print(url)\n",
    "    try:\n",
    "        dataframe = extract_data_from_page(resp)\n",
    "        list_of_dataframes.append(dataframe)\n",
    "    except Exception as e: \n",
    "        print('Failed:',url)\n",
    "        print(e,'\\n------------------------')\n",
    "        if str(e) == 'No Table Found':\n",
    "            fails.append((url,resp))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "have to run throught the failed ones before concatinating the dataframes. \n",
    "\n",
    "Fails happen for 2 reasons:\n",
    "- The page is empty eg: https://www.irelandelection.com/candidate.php?candid=30\n",
    "- The page didnt return anything because I sent so many requests that their server couldnt respond.\n",
    "\n",
    "We send the requests back through again for the failed because of lack of tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fails2 = []\n",
    "tasks = []\n",
    "\n",
    "connector = aiohttp.TCPConnector(limit=60)\n",
    "async with aiohttp.ClientSession(connector=connector,**client_args) as s:\n",
    "    for (url,resp) in fails:\n",
    "        tasks.append(fetch(s,url)) \n",
    "    responses2 = await asyncio.gather(*tasks, return_exceptions=True)\n",
    "\n",
    "\n",
    "for (url,resp) in responses2:\n",
    "    #print(url)\n",
    "    try:\n",
    "        dataframe = extract_data_from_page(resp)\n",
    "        list_of_dataframes.append(dataframe)\n",
    "    except Exception as e: \n",
    "        print('Failed:',url)\n",
    "        print(e,'\\n------------------------')\n",
    "        if str(e) == 'No Table Found':\n",
    "            fails2.append(\n",
    "                (url,resp)\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(fails2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DF = pd.concat(list_of_dataframes)\n",
    "DF.to_parquet('ALL_CANDIDATES.parquet')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For every elections we want\n",
    "- Number of Constituencies\n",
    "- How many consituency do we have vote data on?\n",
    "- What was the quota?\n",
    "- What was the votes/quota in first count?\n",
    "- What was the lowest votes/quota?\n",
    "- What was the highest votes/quota?\n",
    "- Who transfered to who (if you have transfer data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "21492c4fe8720d6af13e280c2b801faa6d368fda036b4f197ba5cbd3f78878df"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
